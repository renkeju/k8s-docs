# 4.8.1 资源需求

下面的是示例中，自主式 Pod 要求为 stress 容器确保 128Mi 的内存及五分之一个CPU核心（200m）资源可用，它运行 stress-ng 镜像启动一个进程（-m 1）进行内存性能压力测试，满载测试时它也会尽可能多的占用CPU资源，另外再启动一个专用的CPU压力测试进程（-c 1）。stress-ng 是一个多功能系统压力测试工具，master/worker 模型，Master 为主进程，负责生成和控制子进程，worker 是负责执行各类特定测试的子进程，例如测试CPU的子进程，以及测试 RAM 的子进程等：

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: stress-pod
spec:
  containers:
  - name: stress
    image: ikubernetes/stress-ng
    command: ["/usr/bin/stress-ng", "-m 1", "-c 1", "-metrics-brief"]
    resources:
      requests:
        memory: "128Mi"
        cpu: "200m"
```

上面的配置清单中，其请求使用的 CPU 资源大小为 200m，这意味着一个CPU核心足以确保其以期望的最快方式运行。另外，配置清单中期望使用的内存大小为128Mi，不过其运行时未必会真的用到这么多。考虑到内存为非压缩性资源，其超出指定的大小再运行时存在被 OOM killer 杀死的可能性，于是请求值也应该就是其理想中使用的内存空间上限。

接下来创建并运行此Pod对其资源限制效果尽心检查。需要特别说明的是，笔者当前使用的系统环境中，每个节点的可用 CPU 核心数均为 8，物理内存空间为 16GB:

```
**[terminal]
**[delimiter $ ]**[command kubectl create -f pod-resources-test.yaml]
pod/stress-pod created
```

而后在 Pod 资源的容器内运行 top 命令观察其 CPU 及内存资源的占用状态，如下所示，其中 {stress-ng-vm} 是执行内存压测的子进程，它默认使用 256m 的内存空间，{stress-ng-cpu} 是执行 CPU 压测的专用子进程：

```
**[terminal]
**[delimiter $ ]**[command kubectl exec stress-pod -- top]
Mem: 1731528K used, 6277504K free, 28504K shrd, 2104K buff, 929196K cached
CPU:  55% usr   7% sys   0% nic  37% idle   0% io   0% irq   0% sirq
Load average: 1.42 0.50 0.21 5/390 18
  PID  PPID USER     STAT   VSZ %VSZ CPU %CPU COMMAND
    8     7 root     R     262m   3%   2  13% {stress-ng-vm} /usr/bin/stress-ng 
    6     1 root     R     6884   0%   1  13% {stress-ng-cpu} /usr/bin/stress-ng
    1     0 root     S     6244   0%   3   0% /usr/bin/stress-ng -m 1 -c 1 --met
......
```

top 命令的输出结果显示，每个测试进程的CPU占用率为 23%（实际为 12.5%），{stress-ng-vm}的内存占用量为 262m（VSZ），此两项资源占用量都远超其请求的用量，原因是stress-ng会在可用的范围内尽可能多的占用相关资源。两个测试线程分布与两个CPU核心资源充裕，虽然容器的内存用量远超 128M，但它依然可运行。一旦资源紧张时，节点仅保证容器有五分之一各CPU核心可用，对于有着8个核心的节点来说，它的占用率为 2.5%，于是每个进程为 1.25%，多占用的资源会被压缩。内存为非可压缩性资源，所以此Pod在内存资源紧张时可能会因为 OOM 被杀死（killed）。

对于压缩型的资源CPU来说，